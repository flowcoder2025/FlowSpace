"use client"

/**
 * useVoiceActivityGate
 *
 * ìŒì„± í™œë™ ê°ì§€(VAD) ê¸°ë°˜ ê²Œì´íŠ¸ í›…
 * - Web Audio APIë¡œ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë ˆë²¨ ì¸¡ì •
 * - inputSensitivity ì„ê³„ê°’ ê¸°ë°˜ ìŒì„± í™œë™ ê°ì§€
 * - íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ + ë””ë°”ìš´ìŠ¤ë¡œ ë¹ˆë²ˆí•œ í† ê¸€ ë°©ì§€
 * - ê°ë„ ì„ê³„ê°’ì— ë„ë‹¬í•˜ì§€ ì•Šìœ¼ë©´ isBelowThreshold = true
 */

import { useState, useEffect, useRef, useCallback } from "react"

const IS_DEV = process.env.NODE_ENV === "development"

interface UseVoiceActivityGateOptions {
  /** ì˜¤ë””ì˜¤ íŠ¸ë™ (ë§ˆì´í¬) */
  audioTrack: MediaStreamTrack | null | undefined
  /** ì…ë ¥ ê°ë„ (0-100, ë‚®ì„ìˆ˜ë¡ ë¯¼ê°) */
  sensitivity: number
  /** VAD í™œì„±í™” ì—¬ë¶€ */
  enabled: boolean
  /** ë””ë°”ìš´ìŠ¤ ì‹œê°„ (ms) - ì„ê³„ê°’ ë¯¸ë§Œ ìœ ì§€ ì‹œê°„ */
  debounceMs?: number
  /** íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ (0-1) - ì„ê³„ê°’ ì „í™˜ ì‹œ ì—¬ìœ  */
  hysteresis?: number
}

interface UseVoiceActivityGateReturn {
  /** í˜„ì¬ ì˜¤ë””ì˜¤ ë ˆë²¨ì´ ì„ê³„ê°’ ë¯¸ë§Œì¸ì§€ */
  isBelowThreshold: boolean
  /** í˜„ì¬ ì˜¤ë””ì˜¤ ë ˆë²¨ (0-100) */
  currentLevel: number
  /** í˜„ì¬ ì ìš©ëœ ì„ê³„ê°’ (0-100) */
  threshold: number
}

/**
 * sensitivity (0-100)ë¥¼ ì‹¤ì œ RMS ì„ê³„ê°’ (0-1)ë¡œ ë³€í™˜
 * - sensitivity 0 = ê°€ì¥ ë¯¼ê° (ì„ê³„ê°’ 0.01)
 * - sensitivity 100 = ê°€ì¥ ë‘”ê° (ì„ê³„ê°’ 0.5)
 *
 * ğŸ“Œ ê°ë„ê°€ ë‚®ì„ìˆ˜ë¡ ì‘ì€ ì†Œë¦¬ì—ë„ ë°˜ì‘í•´ì•¼ í•˜ë¯€ë¡œ
 * sensitivity ê°’ì´ ë‚®ìœ¼ë©´ thresholdë„ ë‚®ì•„ì•¼ í•¨
 */
function sensitivityToThreshold(sensitivity: number): number {
  // 0-100 â†’ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
  const normalized = Math.max(0, Math.min(100, sensitivity)) / 100
  // ì„ê³„ê°’ ë²”ìœ„: 0.01 (ë§¤ìš° ë¯¼ê°) ~ 0.5 (ë§¤ìš° ë‘”ê°)
  // normalized 0 â†’ threshold 0.01
  // normalized 1 â†’ threshold 0.5
  return 0.01 + normalized * 0.49
}

export function useVoiceActivityGate({
  audioTrack,
  sensitivity,
  enabled,
  debounceMs = 150,
  hysteresis = 0.02,
}: UseVoiceActivityGateOptions): UseVoiceActivityGateReturn {
  const [isBelowThreshold, setIsBelowThreshold] = useState(false)
  const [currentLevel, setCurrentLevel] = useState(0)

  // ë‚´ë¶€ ìƒíƒœ refs (ë Œë”ë§ ì‚¬ì´ì— ìœ ì§€)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const debounceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const lastStateRef = useRef<boolean>(false) // ë§ˆì§€ë§‰ ê²Œì´íŠ¸ ìƒíƒœ

  // í˜„ì¬ ì„ê³„ê°’ ê³„ì‚°
  const threshold = sensitivityToThreshold(sensitivity)

  // ì •ë¦¬ í•¨ìˆ˜
  const cleanup = useCallback(() => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = null
    }
    if (debounceTimerRef.current) {
      clearTimeout(debounceTimerRef.current)
      debounceTimerRef.current = null
    }
    if (sourceRef.current) {
      sourceRef.current.disconnect()
      sourceRef.current = null
    }
    if (analyserRef.current) {
      analyserRef.current.disconnect()
      analyserRef.current = null
    }
    // AudioContextëŠ” ì¬ì‚¬ìš©ì„ ìœ„í•´ ìœ ì§€ (closeí•˜ì§€ ì•ŠìŒ)
  }, [])

  // VAD ì¸¡ì • ë£¨í”„
  useEffect(() => {
    // ì¡°ê±´ ì²´í¬
    if (!enabled || !audioTrack || audioTrack.readyState !== "live") {
      cleanup()
      // ğŸ“Œ ë¹„ë™ê¸°ë¡œ setState í˜¸ì¶œ (ESLint react-hooks/set-state-in-effect ê·œì¹™ ëŒ€ì‘)
      void Promise.resolve().then(() => {
        setIsBelowThreshold(false)
        setCurrentLevel(0)
      })
      lastStateRef.current = false
      return
    }

    // AudioContext ìƒì„± (ì¬ì‚¬ìš©)
    if (!audioContextRef.current) {
      try {
        audioContextRef.current = new AudioContext()
      } catch (err) {
        console.error("[useVoiceActivityGate] AudioContext ìƒì„± ì‹¤íŒ¨:", err)
        return
      }
    }

    const audioContext = audioContextRef.current

    // suspended ìƒíƒœë©´ resume
    if (audioContext.state === "suspended") {
      audioContext.resume().catch(console.error)
    }

    // AnalyserNode ìƒì„±
    const analyser = audioContext.createAnalyser()
    analyser.fftSize = 256
    analyser.smoothingTimeConstant = 0.5
    analyserRef.current = analyser

    // MediaStreamSource ìƒì„±
    try {
      const stream = new MediaStream([audioTrack])
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(analyser)
      sourceRef.current = source
    } catch (err) {
      console.error("[useVoiceActivityGate] MediaStreamSource ìƒì„± ì‹¤íŒ¨:", err)
      cleanup()
      return
    }

    // ì¸¡ì • ë°ì´í„° ë²„í¼
    const dataArray = new Float32Array(analyser.fftSize)

    // ì¸¡ì • ë£¨í”„
    const measureLevel = () => {
      if (!analyserRef.current) return

      analyserRef.current.getFloatTimeDomainData(dataArray)

      // RMS ê³„ì‚°
      let sum = 0
      for (let i = 0; i < dataArray.length; i++) {
        sum += dataArray[i] * dataArray[i]
      }
      const rms = Math.sqrt(sum / dataArray.length)

      // 0-100 ë ˆë²¨ë¡œ ë³€í™˜ (í‘œì‹œìš©)
      const levelPercent = Math.min(100, Math.round(rms * 200))
      setCurrentLevel(levelPercent)

      // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì ìš© ì„ê³„ê°’
      const upperThreshold = threshold + hysteresis
      const lowerThreshold = Math.max(0.01, threshold - hysteresis)

      // í˜„ì¬ ìƒíƒœ ê¸°ì¤€ ì„ê³„ê°’ ì„ íƒ
      const effectiveThreshold = lastStateRef.current ? upperThreshold : lowerThreshold

      // ì„ê³„ê°’ ë¹„êµ
      const nowBelowThreshold = rms < effectiveThreshold

      // ë””ë°”ìš´ìŠ¤: ìƒíƒœ ë³€ê²½ ì‹œì—ë§Œ
      if (nowBelowThreshold !== lastStateRef.current) {
        if (debounceTimerRef.current) {
          clearTimeout(debounceTimerRef.current)
        }

        debounceTimerRef.current = setTimeout(() => {
          lastStateRef.current = nowBelowThreshold
          setIsBelowThreshold(nowBelowThreshold)

          if (IS_DEV) {
            console.log("[useVoiceActivityGate] State changed:", {
              isBelowThreshold: nowBelowThreshold,
              rms: rms.toFixed(4),
              threshold: effectiveThreshold.toFixed(4),
              sensitivity,
            })
          }
        }, debounceMs)
      }

      animationFrameRef.current = requestAnimationFrame(measureLevel)
    }

    // ì¸¡ì • ì‹œì‘
    animationFrameRef.current = requestAnimationFrame(measureLevel)

    if (IS_DEV) {
      console.log("[useVoiceActivityGate] Started monitoring:", {
        trackId: audioTrack.id,
        sensitivity,
        threshold: threshold.toFixed(4),
      })
    }

    return cleanup
  }, [audioTrack, enabled, threshold, hysteresis, debounceMs, cleanup, sensitivity])

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ AudioContext ì •ë¦¬
  useEffect(() => {
    return () => {
      cleanup()
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(() => {})
        audioContextRef.current = null
      }
    }
  }, [cleanup])

  return {
    isBelowThreshold,
    currentLevel,
    threshold: Math.round(threshold * 100),
  }
}
