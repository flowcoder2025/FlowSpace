"use client"

/**
 * useAudioGateProcessor
 *
 * AudioWorklet ê¸°ë°˜ ì „ë¬¸ê¸‰ ë…¸ì´ì¦ˆ ê²Œì´íŠ¸ í›…
 * - ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ (ë©”ì¸ ìŠ¤ë ˆë“œ ì°¨ë‹¨ ì—†ìŒ)
 * - ë¶€ë“œëŸ¬ìš´ Attack/Release ì—”ë²¨ë¡œí”„
 * - íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ë¡œ ì±„í„°ë§ ë°©ì§€
 *
 * ì‚¬ìš©ë²•:
 * const { processedTrack, currentLevel, isGateOpen, setThreshold } = useAudioGateProcessor({
 *   inputTrack: localAudioTrack,
 *   sensitivity: audioSettings.inputSensitivity,
 *   enabled: true,
 * })
 */

import { useState, useEffect, useRef, useCallback } from "react"

const IS_DEV = process.env.NODE_ENV === "development"

interface UseAudioGateProcessorOptions {
  /** ì…ë ¥ ì˜¤ë””ì˜¤ íŠ¸ë™ (ë§ˆì´í¬) */
  inputTrack: MediaStreamTrack | null
  /** ì…ë ¥ ê°ë„ (0-100, ë‚®ì„ìˆ˜ë¡ ë¯¼ê°) - 0ì´ë©´ ê²Œì´íŠ¸ ë¹„í™œì„±í™” */
  sensitivity: number
  /** ê²Œì´íŠ¸ í™œì„±í™” ì—¬ë¶€ */
  enabled: boolean
  /** Attack ì‹œê°„ (ì´ˆ) - ê²Œì´íŠ¸ ì—´ë¦¼ ì†ë„ */
  attackTime?: number
  /** Release ì‹œê°„ (ì´ˆ) - ê²Œì´íŠ¸ ë‹«í˜ ì†ë„ */
  releaseTime?: number
}

interface UseAudioGateProcessorReturn {
  /** ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ íŠ¸ë™ (LiveKitì— ì „ë‹¬ìš©) */
  processedTrack: MediaStreamTrack | null
  /** í˜„ì¬ ì˜¤ë””ì˜¤ ë ˆë²¨ (0-100) */
  currentLevel: number
  /** í˜„ì¬ ê²Œì´íŠ¸ ìƒíƒœ (ì—´ë¦¼/ë‹«í˜) */
  isGateOpen: boolean
  /** í˜„ì¬ ê²Œì¸ ê°’ (0-1) */
  currentGain: number
  /** ì„ê³„ê°’ ì„¤ì • */
  setThreshold: (sensitivity: number) => void
  /** ê²Œì´íŠ¸ í™œì„±í™”/ë¹„í™œì„±í™” */
  setEnabled: (enabled: boolean) => void
  /** ì´ˆê¸°í™” ì™„ë£Œ ì—¬ë¶€ */
  isInitialized: boolean
  /** ì—ëŸ¬ ìƒíƒœ */
  error: string | null
}

export function useAudioGateProcessor({
  inputTrack,
  sensitivity,
  enabled,
  attackTime = 0.01,
  releaseTime = 0.1,
}: UseAudioGateProcessorOptions): UseAudioGateProcessorReturn {
  // ìƒíƒœ
  const [processedTrack, setProcessedTrack] = useState<MediaStreamTrack | null>(null)
  const [currentLevel, setCurrentLevel] = useState(0)
  const [isGateOpen, setIsGateOpen] = useState(false)
  const [currentGain, setCurrentGain] = useState(1)
  const [isInitialized, setIsInitialized] = useState(false)
  const [error, setError] = useState<string | null>(null)

  // Refs for audio nodes
  const audioContextRef = useRef<AudioContext | null>(null)
  const sourceNodeRef = useRef<MediaStreamAudioSourceNode | null>(null)
  const workletNodeRef = useRef<AudioWorkletNode | null>(null)
  const destinationNodeRef = useRef<MediaStreamAudioDestinationNode | null>(null)
  const isCleaningUpRef = useRef(false)

  // ğŸ“Œ sensitivityë¥¼ refë¡œ ì¶”ì  (0 â†” non-zero ì „í™˜ ê°ì§€ìš©)
  // sensitivity ìì²´ëŠ” íŒŒì´í”„ë¼ì¸ ì¬ìƒì„±ì„ íŠ¸ë¦¬ê±°í•˜ì§€ ì•ŠìŒ (workletì— ë©”ì‹œì§€ë§Œ ì „ì†¡)
  const prevSensitivityRef = useRef(sensitivity)
  const isGateEnabledRef = useRef(sensitivity > 0) // ê²Œì´íŠ¸ í™œì„±í™” ì—¬ë¶€ (0ì´ë©´ ë¹„í™œì„±í™”)

  // ğŸ“Œ ì´ˆê¸°í™” ìƒíƒœë¥¼ refë¡œ ì¶”ì  (ì˜ì¡´ì„± ë°°ì—´ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•¨)
  // isInitializedê°€ ì˜ì¡´ì„±ì— ìˆìœ¼ë©´ ì´ˆê¸°í™” ì™„ë£Œ ì‹œ cleanupì´ í˜¸ì¶œë˜ì–´ íŒŒì´í”„ë¼ì¸ì´ íŒŒê´´ë¨
  const isInitializedRef = useRef(false)

  // Cleanup í•¨ìˆ˜
  const cleanup = useCallback(() => {
    if (isCleaningUpRef.current) return
    isCleaningUpRef.current = true

    if (IS_DEV) {
      console.log("[useAudioGateProcessor] Cleaning up...")
    }

    // Source node disconnect
    if (sourceNodeRef.current) {
      try {
        sourceNodeRef.current.disconnect()
      } catch (e) {
        // Ignore disconnect errors
      }
      sourceNodeRef.current = null
    }

    // Worklet node disconnect
    if (workletNodeRef.current) {
      try {
        workletNodeRef.current.disconnect()
        workletNodeRef.current.port.close()
      } catch (e) {
        // Ignore disconnect errors
      }
      workletNodeRef.current = null
    }

    // Destination node
    if (destinationNodeRef.current) {
      destinationNodeRef.current = null
    }

    // Don't close AudioContext - reuse it
    setProcessedTrack(null)
    setIsInitialized(false)
    isInitializedRef.current = false  // ğŸ“Œ refë„ ë™ê¸°í™”
    isCleaningUpRef.current = false
  }, [])

  // ğŸ“Œ 0 â†” non-zero ì „í™˜ ê°ì§€ (íŒŒì´í”„ë¼ì¸ ìƒì„±/ì •ë¦¬ íŠ¸ë¦¬ê±°)
  // sensitivity ê°’ ìì²´ì˜ ë³€ê²½(ì˜ˆ: 30â†’50)ì€ íŒŒì´í”„ë¼ì¸ì„ ì¬ìƒì„±í•˜ì§€ ì•ŠìŒ
  const shouldCreatePipeline = sensitivity > 0

  // ref ì—…ë°ì´íŠ¸
  useEffect(() => {
    prevSensitivityRef.current = sensitivity
    isGateEnabledRef.current = sensitivity > 0
  }, [sensitivity])

  // AudioWorklet ì´ˆê¸°í™” ë° íŒŒì´í”„ë¼ì¸ ì„¤ì •
  // ğŸ“Œ í•µì‹¬ ìˆ˜ì •: sensitivity ìì²´ëŠ” ì˜ì¡´ì„±ì—ì„œ ì œì™¸
  // - 0 â†’ non-zero ì „í™˜ ì‹œì—ë§Œ íŒŒì´í”„ë¼ì¸ ìƒì„± (shouldCreatePipeline ì‚¬ìš©)
  // - non-zero â†’ non-zero ë³€ê²½ì€ ë‘ ë²ˆì§¸ effectì—ì„œ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬
  useEffect(() => {
    // ğŸ“Œ sensitivityê°€ 0ì´ë©´ AudioWorklet íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ
    // ì´ ê²½ìš° ì›ë³¸ LiveKit íŠ¸ë™ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©ë¨ (ì„±ëŠ¥ ìµœì í™” + í˜¸í™˜ì„±)
    if (!shouldCreatePipeline) {
      cleanup()
      if (IS_DEV) {
        console.log("[useAudioGateProcessor] Sensitivity is 0, skipping AudioWorklet pipeline")
      }
      return
    }

    // ì…ë ¥ íŠ¸ë™ì´ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ì •ë¦¬
    if (!inputTrack || inputTrack.readyState !== "live") {
      cleanup()
      return
    }

    // ğŸ“Œ ì´ë¯¸ ì´ˆê¸°í™”ëœ íŒŒì´í”„ë¼ì¸ì´ ìˆìœ¼ë©´ ì¬ìƒì„±í•˜ì§€ ì•ŠìŒ
    // (sensitivity ë³€ê²½ì€ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬)
    // refë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„± ë°°ì—´ì—ì„œ ì œì™¸ (isInitializedê°€ ì˜ì¡´ì„±ì— ìˆìœ¼ë©´ cleanup í˜¸ì¶œë¨)
    if (isInitializedRef.current && workletNodeRef.current && sourceNodeRef.current) {
      if (IS_DEV) {
        console.log("[useAudioGateProcessor] Pipeline already initialized, skipping recreation")
      }
      return
    }

    let isMounted = true

    const initializeWorklet = async () => {
      try {
        setError(null)

        // AudioContext ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
        if (!audioContextRef.current) {
          audioContextRef.current = new AudioContext()
        }

        const audioContext = audioContextRef.current

        // Suspended ìƒíƒœë©´ resume
        if (audioContext.state === "suspended") {
          await audioContext.resume()
        }

        // AudioWorklet ëª¨ë“ˆ ë¡œë“œ (í•œ ë²ˆë§Œ)
        try {
          await audioContext.audioWorklet.addModule("/audio-worklets/noise-gate-processor.js")
        } catch (e) {
          // ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ì—ëŸ¬ ë¬´ì‹œ
          if (!(e instanceof DOMException && e.name === "InvalidStateError")) {
            throw e
          }
        }

        if (!isMounted) return

        // ì…ë ¥ ìŠ¤íŠ¸ë¦¼ ìƒì„±
        const inputStream = new MediaStream([inputTrack])
        const sourceNode = audioContext.createMediaStreamSource(inputStream)
        sourceNodeRef.current = sourceNode

        // AudioWorkletNode ìƒì„±
        const workletNode = new AudioWorkletNode(audioContext, "noise-gate-processor")
        workletNodeRef.current = workletNode

        // Workletì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹  (ë ˆë²¨ ë¦¬í¬íŠ¸)
        workletNode.port.onmessage = (event) => {
          if (!isMounted) return

          const { type, data } = event.data
          if (type === "levelReport") {
            setCurrentLevel(data.level)
            setIsGateOpen(data.isGateOpen)
            setCurrentGain(data.gain)
          }
        }

        // ì¶œë ¥ destination ìƒì„±
        const destinationNode = audioContext.createMediaStreamDestination()
        destinationNodeRef.current = destinationNode

        // ë…¸ë“œ ì—°ê²°: Source â†’ Worklet â†’ Destination
        sourceNode.connect(workletNode)
        workletNode.connect(destinationNode)

        // ì²˜ë¦¬ëœ íŠ¸ë™ ì¶”ì¶œ
        const processedAudioTrack = destinationNode.stream.getAudioTracks()[0]
        if (!processedAudioTrack) {
          throw new Error("Failed to get processed audio track")
        }

        setProcessedTrack(processedAudioTrack)
        setIsInitialized(true)
        isInitializedRef.current = true  // ğŸ“Œ refë„ ë™ê¸°í™”

        // ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • (í˜„ì¬ sensitivity ê°’ ì‚¬ìš©)
        const currentSensitivity = prevSensitivityRef.current
        workletNode.port.postMessage({ type: "setThreshold", data: currentSensitivity })
        workletNode.port.postMessage({ type: "setEnabled", data: enabled && currentSensitivity > 0 })
        workletNode.port.postMessage({ type: "setAttackTime", data: attackTime })
        workletNode.port.postMessage({ type: "setReleaseTime", data: releaseTime })

        if (IS_DEV) {
          console.log("[useAudioGateProcessor] Initialized successfully", {
            sensitivity: currentSensitivity,
            enabled,
            attackTime,
            releaseTime,
          })
        }
      } catch (err) {
        console.error("[useAudioGateProcessor] Initialization error:", err)
        setError(err instanceof Error ? err.message : "Unknown error")
        cleanup()
      }
    }

    initializeWorklet()

    return () => {
      isMounted = false
      cleanup()
    }
    // ğŸ“Œ isInitialized ì œê±°! ì˜ì¡´ì„±ì— ìˆìœ¼ë©´ ì´ˆê¸°í™” ì™„ë£Œ ì‹œ cleanupì´ í˜¸ì¶œë¨
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [inputTrack, shouldCreatePipeline, cleanup, attackTime, releaseTime, enabled])

  // Sensitivity ë³€ê²½ ì‹œ Workletì— ì „ë‹¬
  useEffect(() => {
    if (workletNodeRef.current && isInitialized) {
      workletNodeRef.current.port.postMessage({ type: "setThreshold", data: sensitivity })
      // sensitivityê°€ 0ì´ë©´ ê²Œì´íŠ¸ ë¹„í™œì„±í™”
      workletNodeRef.current.port.postMessage({
        type: "setEnabled",
        data: enabled && sensitivity > 0,
      })

      if (IS_DEV) {
        console.log("[useAudioGateProcessor] Updated threshold:", { sensitivity, enabled })
      }
    }
  }, [sensitivity, enabled, isInitialized])

  // setThreshold ì½œë°±
  const setThreshold = useCallback((newSensitivity: number) => {
    if (workletNodeRef.current) {
      workletNodeRef.current.port.postMessage({ type: "setThreshold", data: newSensitivity })
    }
  }, [])

  // setEnabled ì½œë°±
  const setEnabled = useCallback((newEnabled: boolean) => {
    if (workletNodeRef.current) {
      workletNodeRef.current.port.postMessage({ type: "setEnabled", data: newEnabled })
    }
  }, [])

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ AudioContext ì •ë¦¬
  useEffect(() => {
    return () => {
      cleanup()
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(() => {})
        audioContextRef.current = null
      }
    }
  }, [cleanup])

  return {
    processedTrack,
    currentLevel,
    isGateOpen,
    currentGain,
    setThreshold,
    setEnabled,
    isInitialized,
    error,
  }
}
